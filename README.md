# Awesome-o1-for-code


o1 like LLM for code intelligence.

#### RAG && Reasoning
1. [**RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement.**](https://arxiv.org/abs/2412.12881) *Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, Tao Zhang
.* Arxiv 2024.12.17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

2. [**Outcome-Refining Process Supervision for Code Generation.**](https://arxiv.org/abs/2403.03163) *Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang
.* Arxiv 2024.12.19. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/zhuohaoyu/ORPS)](https://github.com/zhuohaoyu/ORPS)

3. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319) *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao
.* Arxiv 2024.12.24. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/HJYao00/Mulberry)](https://github.com/HJYao00/Mulberry)

4. [**Search-o1: Agentic Search-Enhanced Large Reasoning Models.**](https://arxiv.org/pdf/2501.05366) *Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou
.* Arxiv 2025.1.9. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/sunnynexus/Search-o1)](https://github.com/sunnynexus/Search-o1)


5. [**CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction.**](https://arxiv.org/abs/2502.07316) *Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He
.* Arxiv 2025.2.11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/CodeIO)](https://github.com/hkust-nlp/CodeIO)


6. [**LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!.**](https://arxiv.org/abs/2502.07316) *Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica.* Arxiv 2025.2.11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)](https://github.com/NovaSky-AI/SkyThought)

7. [**Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?.**](https://arxiv.org/abs/2502.19361) *Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, Bo Zheng.* Arxiv 2025.2.26. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/OpenStellarTeam/DeltaBench)](https://github.com/OpenStellarTeam/DeltaBench)


8. [**The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models.**](https://arxiv.org/abs/2503.02875) *Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen, Benyou Wang, Zhaopeng Tu, Haitao Mi, Dong Yu.* Arxiv 2025.03.04.





#### Overthinking

1. [**To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning.**](https://arxiv.org/abs/2409.12183) *Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett
.* Arxiv 2024.9.18.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/Zayne-sprague/To-CoT-or-not-to-CoT)](https://github.com/Zayne-sprague/To-CoT-or-not-to-CoT)

2. [**Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning.**](https://arxiv.org/abs/2412.01113) *Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui
.* Arxiv 2024.12.2.

3. [**Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs.**](https://arxiv.org/abs/2412.21187) *Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu
.* Arxiv 2024.12.30.

4. [**The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks.**](https://arxiv.org/abs/2502.08235) *Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez
.* Arxiv 2025.2.12.

5. [**Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs.**](https://arxiv.org/abs/2502.10858) *Zongqian Wu, Tianyu Li, Baoduo Xu, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng.* Arxiv 2025.2.15.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/zongqianwu/breadth)](https://github.com/zongqianwu/breadth)

6. [**SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities.**](https://arxiv.org/abs/2502.12025) *Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran
.* Arxiv 2025.2.17.





#### Token Compression

1. [**LookupViT: Compressing visual information to a limited number of tokens**](https://arxiv.org/abs/2407.12753) *Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, Sujoy Paul
.* ECCV 2024 Arxiv 2024.7.17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


2. [**ShowUI: One Vision-Language-Action Model for GUI Visual Agent**](https://arxiv.org/abs/2412.21187) *Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou
.* Arxiv 2024.11.26. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/showlab/ShowUI)](https://github.com/showlab/ShowUI)

3. [**Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration**](https://arxiv.org/pdf/2411.17686) *Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, Siteng Huang
.* Arxiv 2024.11.26. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/kawhiiiileo/FiCoCo)](https://github.com/kawhiiiileo/FiCoCo)


4. [**ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models**](https://arxiv.org/pdf/2412.00447) *Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, Yansong Tang
.* Arxiv 2024.11.30. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


5. [**VisionZip: Longer is Better but Not Necessary in Vision Language Models**](https://arxiv.org/abs/2412.04467) *Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, Jiaya Jia
.* Arxiv 2024.12.5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/VisionZip)](https://github.com/dvlab-research/VisionZip)


6. [**Perception Tokens Enhance Visual Reasoning in Multimodal Language Models**](https://arxiv.org/abs/2412.03548) *Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G. Shapiro, Ranjay Krishna
.* Arxiv 2024.12.8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/mahtabbigverdi/Aurora/tree/main)](https://github.com/mahtabbigverdi/Aurora)


7. [**[CLS] Token Tells Everything Needed for Training-free Efficient MLLMs**](https://arxiv.org/abs/2412.05819) *Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, Guiguang Ding
.* Arxiv 2024.12.8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/THU-MIG/VTC-CLS)](https://github.com/THU-MIG/VTC-CLS)


8. [**Token-Budget-Aware LLM Reasoning**](https://arxiv.org/abs/2412.18547) *Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen
.* Arxiv 2024.12.24. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/GeniusHTX/TALE)](https://github.com/GeniusHTX/TALE)

9. [**CoT-Valve: Length-Compressible Chain-of-Thought Tuning**](https://arxiv.org/abs/2502.09601) *Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, Xinchao Wang
.* Arxiv 2025.02.13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/horseee/CoT-Valve)](https://github.com/horseee/CoT-Valve)

10. [**TokenSkip: Controllable Chain-of-Thought Compression in LLMs**](https://arxiv.org/abs/2502.12067) *Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li
.* Arxiv 2025.02.17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/hemingkx/TokenSkip)](https://github.com/hemingkx/TokenSkip)

11. [**Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More**](https://arxiv.org/abs/2502.11494) *Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang
.* Arxiv 2025.02.17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/ZichenWen1/DART)](https://github.com/ZichenWen1/DART)

12. [**MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs**](https://arxiv.org/abs/2502.17422) *Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski
.* Arxiv 2025.02.24. (ICLR 2025) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/saccharomycetes/mllms_know)](https://github.com/saccharomycetes/mllms_know)


13. [**DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models**](https://arxiv.org/abs/2503.02175) *Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, Yong Zhang
.* Arxiv 2025.03.04. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/vbdi/divprune)](https://github.com/vbdi/divprune)




#### Efficient Code Generation
1. [**When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention**](https://arxiv.org/abs/2412.21187) *Lianghong Guo, Yanlin Wang, Ensheng Shi, Wanjun Zhong, Hongyu Zhang, Jiachi Chen, Ruikai Zhang, Yuchi Ma, Zibin Zheng
.* ISSTA 2024 (2024.7.29). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/DeepSoftwareAnalytics/CodeFast)](https://github.com/DeepSoftwareAnalytics/CodeFast)

2. [**Rethinking Repetition Problems of LLMs in Code Generation**](https://arxiv.org/abs/2505.10402) *Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li
.* ACL 2025 (2025.5.15). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/LYC127/RPG)](https://github.com/LYC127/RPG)
   


#### Interpretability
1. [**Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?**](https://arxiv.org/abs/2306.01220) *Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, Tianyi Zhang
.* FSE 2024 (2023.6.2). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/BonanKou/Attention-Alignment-Empirical-Study)](https://github.com/BonanKou/Attention-Alignment-Empirical-Study)


#### Others
1. [**Proof Automation with Large Language Models**](https://dl.acm.org/doi/10.1145/3691620.3695521) *Minghai Lu, Benjamin Delaware, Tianyi Zhang
.* ASE 2024. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/lachinygair/PALM)](https://github.com/lachinygair/PALM)


2. [**Attention Is All You Need for LLM-based Code Vulnerability Localization**](https://dl.acm.org/doi/10.1145/3691620.3695521) *Yue Li, Xiao Li, Hao Wu, Yue Zhang, Xiuzhen Cheng, Sheng Zhong, Fengyuan Xu
.*


Some open-source web links:
- react: https://v0.dev/
- vue: https://github.com/vuejs/awesome-vue
- angular:
  -- https://www.reddit.com/r/Angular2/comments/uq2dg8/what_are_some_good_open_source_angular_projects/
  -- https://github.com/coryrylan/ng-projects
  -- https://flatlogic.com/blog/top-angular-open-source-projects/

general: 
- https://github.com/sdil/open-production-web-projects
- https://github.com/makinwab/awesome-opensource-webapps
- https://github.com/unicodeveloper/awesome-opensource-apps

