# Awesome-o1-for-code


o1 like LLM for code intelligence.

#### RAG && Reasoning
1. [**RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement.**](https://arxiv.org/abs/2412.12881) *Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, Tao Zhang
.* Arxiv 2024.12.17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

2. [**Outcome-Refining Process Supervision for Code Generation.**](https://arxiv.org/abs/2403.03163) *Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang
.* Arxiv 2024.12.19. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/zhuohaoyu/ORPS)](https://github.com/zhuohaoyu/ORPS)

3. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319) *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao
.* Arxiv 2024.12.24. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/HJYao00/Mulberry)](https://github.com/HJYao00/Mulberry)

4. [**Search-o1: Agentic Search-Enhanced Large Reasoning Models.**](https://arxiv.org/pdf/2501.05366) *Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou
.* Arxiv 2025.1.9. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/sunnynexus/Search-o1)](https://github.com/sunnynexus/Search-o1)


#### Overthinking

1. [**Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning.**](https://arxiv.org/abs/2412.01113) *Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui
.* Arxiv 2024.12.2.

2. [**Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs.**](https://arxiv.org/abs/2412.21187) *Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu
.* Arxiv 2024.12.30.

3. [**The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks.**](https://arxiv.org/abs/2502.08235) *Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez
.* Arxiv 2025.2.12.

4. [**Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs.**](https://arxiv.org/abs/2502.10858) *Zongqian Wu, Tianyu Li, Baoduo Xu, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng.* Arxiv 2025.2.15.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/zongqianwu/breadth)](https://github.com/zongqianwu/breadth)


5. [**SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities.**](https://arxiv.org/abs/2502.12025) *Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran
.* Arxiv 2025.2.17.



Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs


#### Token Compression

1. [**LookupViT: Compressing visual information to a limited number of tokens**](https://arxiv.org/abs/2407.12753) *Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, Sujoy Paul
.* ECCV 2024 Arxiv 2024.7.17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


1. [**ShowUI: One Vision-Language-Action Model for GUI Visual Agent**](https://arxiv.org/abs/2412.21187) *Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou
.* Arxiv 2024.11.26. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/showlab/ShowUI)](https://github.com/showlab/ShowUI)

2. [**Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration**](https://arxiv.org/pdf/2411.17686) *Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, Siteng Huang
.* Arxiv 2024.11.26. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/kawhiiiileo/FiCoCo)](https://github.com/kawhiiiileo/FiCoCo)


3. [**ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models**](https://arxiv.org/pdf/2412.00447) *Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, Yansong Tang
.* Arxiv 2024.11.30. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


4. [**VisionZip: Longer is Better but Not Necessary in Vision Language Models**](https://arxiv.org/abs/2412.04467) *Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, Jiaya Jia
.* Arxiv 2024.12.5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/VisionZip)](https://github.com/dvlab-research/VisionZip)


5. [**Perception Tokens Enhance Visual Reasoning in Multimodal Language Models**](https://arxiv.org/abs/2412.03548) *Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G. Shapiro, Ranjay Krishna
.* Arxiv 2024.12.8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/mahtabbigverdi/Aurora/tree/main)](https://github.com/mahtabbigverdi/Aurora)






#### Efficient Code Generation
1. [**When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention**](https://arxiv.org/abs/2412.21187) *Lianghong Guo, Yanlin Wang, Ensheng Shi, Wanjun Zhong, Hongyu Zhang, Jiachi Chen, Ruikai Zhang, Yuchi Ma, Zibin Zheng
.* ISSTA 2024 (2024.7.29). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/DeepSoftwareAnalytics/CodeFast)](https://github.com/DeepSoftwareAnalytics/CodeFast)


#### Interpretability
1. [**Do Large Language Models Pay Similar Attention Like Human Programmers When Generating Code?**](https://arxiv.org/abs/2306.01220) *Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, Tianyi Zhang
.* FSE 2024 (2023.6.2). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/BonanKou/Attention-Alignment-Empirical-Study)](https://github.com/BonanKou/Attention-Alignment-Empirical-Study)


#### Others
1. [**Proof Automation with Large Language Models**](https://dl.acm.org/doi/10.1145/3691620.3695521) *Minghai Lu, Benjamin Delaware, Tianyi Zhang
.* ASE 2024. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [![GitHub Repo stars](https://img.shields.io/github/stars/lachinygair/PALM)](https://github.com/lachinygair/PALM)


2. [**Attention Is All You Need for LLM-based Code Vulnerability Localization**](https://dl.acm.org/doi/10.1145/3691620.3695521) *Yue Li, Xiao Li, Hao Wu, Yue Zhang, Xiuzhen Cheng, Sheng Zhong, Fengyuan Xu
.*


